@inproceedings{
kandemir2022evidential,
abbr={ICLR},
title={Evidential {T}uring Processes },
author={Kandemir, M. and Akg{\"u}l, A. and Haussmann, M. and Unal, G.},
booktitle={ICLR-International Conference on Learning Representations},
year={2022},
selected={true}
}

@inproceedings{ozer2022fl,  
title={How to combine {V}ariational {B}ayesian {N}etworks in {F}ederated {L}earning},  
author = {Ozer, A. and Buldu, K.B. and Akgül, A. and Unal, G.},  
booktitle={Workshop on Federated Learning: Recent Advances and New Challenges (in Conjunction with NeurIPS 2022)},  
year={2022},  
abbr={FL-NeurIPS},
url={https://openreview.net/forum?id=AkPwb9dvAlP},  
html={https://openreview.net/forum?id=AkPwb9dvAlP}
}

@article{MERTAN2022103441,
abbr={DSP},
title = {Single image depth estimation: An overview},
journal = {Digital Signal Processing},
volume = {123},
pages = {103441},
year = {2022},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2022.103441},
url = {https://www.sciencedirect.com/science/article/pii/S1051200422000586},
author = {Mertan, A. and Duff, D.J. and Unal, G.},
keywords = {SIDE, Single image depth estimation, Depth from a single image, Review, Survey},
abstract = {We review solutions to the problem of depth estimation, arguably the most important subtask in scene understanding. We focus on the single image depth estimation problem. Due to its properties, the single image depth estimation problem is currently best tackled with machine learning methods, most successfully with convolutional neural networks. We provide an overview of the field by examining key works. We examine non-deep learning approaches that mostly predate deep learning and utilize hand-crafted features and assumptions, and more recent works that mostly use deep learning techniques. The single image depth estimation problem is tackled in a supervised fashion with absolute or relative depth information acquired from human or sensor-labeled data, or in an unsupervised way using unlabeled stereo images or video datasets. We also study multitask approaches that combine the depth estimation problem with related tasks such as semantic segmentation and surface normal estimation. Finally, we discuss investigations into the mechanisms, principles, and failure cases of contemporary solutions.}
}


@article{BAYKAL2022108244,
abbr={PR},
title = {Exploring DeshuffleGANs in Self-Supervised Generative Adversarial Networks},
journal = {Pattern Recognition},
volume = {122},
pages = {108244},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108244},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321004167},
author = {Baykal, G. and Ozcelik, F. and Unal, G.},
keywords = {Self-Supervised generative adversarial networks, Generative adversarial networks, Self-supervised learning, DeshuffleGANs, Deshuffling},
abstract = {Generative Adversarial Networks (GANs) have become the most used networks towards solving the problem of image generation. Self-supervised GANs are later proposed to avoid the catastrophic forgetting of the discriminator and to improve the image generation quality without needing the class labels. However, the generalizability of the self-supervision tasks on different GAN architectures is not studied before. To that end, we extensively analyze the contribution of a previously proposed self-supervision task, deshuffling of the DeshuffleGANs in the generalizability context. We assign the deshuffling task to two different GAN discriminators and study the effects of the task on both architectures. We extend the evaluations compared to the previously proposed DeshuffleGANs on various datasets. We show that the DeshuffleGAN obtains the best FID results for several datasets compared to the other self-supervised GANs. Furthermore, we compare the deshuffling with the rotation prediction that is firstly deployed to the GAN training and demonstrate that its contribution exceeds the rotation prediction. We design the conditional DeshuffleGAN called cDeshuffleGAN to evaluate the quality of the learnt representations. Lastly, we show the contribution of the self-supervision tasks to the GAN training on the loss landscape and present that the effects of these tasks may not be cooperative to the adversarial training in some settings. Our code can be found at https://github.com/gulcinbaykal/DeshuffleGAN.}
}

@article{SAHIN2022610,
abbr={CG},
title = {{ODFNet}: Using orientation distribution functions to characterize 3D point clouds},
journal = {Computers \& Graphics},
volume = {102},
pages = {610-618},
year = {2022},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2021.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S0097849321001801},
author = {Sahin, Y.H. and Mertan, A. and Unal, G.},
keywords = {Deep learning, Point cloud, Shape analysis},
abstract = {Learning new representations of 3D point clouds is an active research area in 3D vision, as the order-invariant point cloud structure still presents challenges for the design of neural network architectures. Recent work explored learning global, local, or multi-scale features for point clouds. However, none of the earlier methods focused on capturing contextual shape information by analyzing local orientation distributions of points. In this paper, we use point orientation distributions around a point in order to obtain an expressive local neighborhood representation for point clouds. We achieve this by dividing the spherical neighborhood of a given point into predefined cone volumes, and statistics inside each volume are used as point features. In this way, a local patch can be represented not only by the selected point’s nearest neighbors, but also by considering a point density distribution defined along multiple orientations around the point. We are then able to construct an orientation distribution function (ODF) neural network that makes use of an ODFBlock which relies on MLP (multi-layer perceptron) layers. The new ODFNet model achieves state-of-the-art accuracy for object classification on ModelNet40 and ScanObjectNN datasets, and segmentation on ShapeNet and S3DIS datasets.}
}
@InProceedings{10.1007/978-3-031-09037-0_10,
abbr={ICPRAI},
author="Oncel, F. and Ayg{\"u}n, M. and Baykal, G. and Unal, G.",
editor="El Yacoubi, M. and Granger, E. and Yuen, P.C. and Pal, U. and Vincent, N.",
title="{UGQE}: Uncertainty Guided Query Expansion",
booktitle="Pattern Recognition and Artificial Intelligence",
year="2022",
publisher="Springer International",
address="Cham",
pages="109--120",
abstract="Query expansion is a standard technique in image retrieval, which enriches the original query by capturing various features from relevant images and further aggregating these features to create an expanded query. In this work, we present a new framework, which is based on incorporating uncertainty estimation on top of a self attention mechanism during the expansion procedure. An uncertainty network provides added information on the images that are relevant to the query, in order to increase the expressiveness of the expanded query. Experimental results demonstrate that integrating uncertainty information into a transformer network can improve the performance in terms of mean Average Precision (mAP) on standard image retrieval datasets in comparison to existing methods. Moreover, our approach is the first one that incorporates uncertainty in aggregation of information in a query expansion procedure.",
isbn="978-3-031-09037-0"
}

@InProceedings{10.1007/978-981-19-1280-1_33,
author="G{\"u}ne{\c{s}}, M.C. and Mertan, A. and Sahin, Y.H. and Unal, G. and {\"O}zkar, M.",
editor="Gerber, D. and Pantazis, E. and Bogosian, B. and Nahmad, A. and Miltiadis, C.",
title="Synthesizing Point Cloud Data Set for Historical Dome Systems",
booktitle="Computer-Aided Architectural Design. Design Imperatives: The Future is Now",
year="2022",
publisher="Springer Singapore",
pages="538--554",
abstract="This paper offers a workflow for generating synthetic point cloud data sets to be used in deep learning algorithms in tasks of modeling historical architectural elements. Documentation of cultural heritage is a time-consuming process that requires high precision. Computational and semi-automatic tools enhance conventional methods to shorten the duration of the documentation phase and increase the accuracy of the output. Photogrammetry and laser scanning are how geometrical data is acquired and delivered as a point cloud with position, color, and optionally normal vector information. Segmenting architectural elements based on our interpretations of this data is possible using deep neural networks but is limited when, despite the millions of points from one building, the data is insufficient in terms of variance and quantity. To overcome this limitation, we propose a semi-automatic synthetic data set generation using parametric definitions of historic architectural elements. We create a synthetic dataset, namely the Historical Dome Dataset (HDD), consisting of nearly 1000 dome systems with four semantic classes. We quantitatively and qualitatively analyze the usefulness of the HDD by training a number of modern neural networks on it. Our method of synthesizing point clouds can quickly be adapted into similar cultural heritage projects to prepare relevant data to accurately train deep neural networks and process the collected cultural heritage data.",
isbn="978-981-19-1280-1"
}



@article{DEMIR2021103826,
title = {Detecting visual design principles in art and architecture through deep convolutional neural networks},
journal = {Automation in Construction},
volume = {130},
pages = {103826},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103826},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521002776},
author = {Demir, G. and Çekmiş, A. and Yeşilkaynak, V.B. and Unal, G.},
keywords = {Visual analysis, Visual design principles, Image recognition, Computer vision, Deep learning, Deep convolutional neural network (CNN)},
abstract = {Visual design is associated with the use of some basic design elements and principles. Those are applied by the designers in the various disciplines for aesthetic purposes, relying on an intuitive and subjective process. Thus, numerical analysis of design visuals and disclosure of the aesthetic value embedded in them are considered as hard. However, it has become possible with emerging artificial intelligence technologies. This research aims at a neural network model, which recognizes and classifies the design principles over different domains. The domains include artwork produced since the late 20th century; professional photos; and facade pictures of contemporary buildings. The data collection and curation processes, including the production of computationally-based synthetic dataset, is genuine. The proposed model learns from the knowledge of myriads of original designs, by capturing the underlying shared patterns. It is expected to consolidate design processes by providing an aesthetic evaluation of the visual compositions with objectivity.}
}
@ARTICLE{Ozcelik2021,
  author={Ozcelik, F. and Alganci, U. and Sertel, E. and Unal, G.},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Rethinking {CNN}-Based Pansharpening: Guided Colorization of Panchromatic Images via {GAN}s}, 
  year={2021},
  volume={59},
  number={4},
  pages={3486-3501},
  doi={10.1109/TGRS.2020.3010441}}

@article{KAVUR2021101950,
title = {CHAOS Challenge - combined ({CT-MR}) healthy abdominal organ segmentation},
journal = {Medical Image Analysis},
volume = {69},
pages = {101950},
year = {2021},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2020.101950},
url = {https://www.sciencedirect.com/science/article/pii/S1361841520303145},
author = {Kavur, A.E. and Gezer, N. S. and  Barış, M. and Aslan, S. and Conze, P-H. and Groza, V. and Pham, D.D. and Chatterjee, S. and Ernst, P.  and Özkan, S. and Baydar, B.  and Lachinov,  D. and Han, S. and Pauli, J. and Isensee, F. and Perkonigg, M. and Sathish, R. and Rajan, R. and Sheet, D. and Dovletov, G. and Speck, O. and Nürnberger, A. and Maier-Hein, K. H. and Akar, G.B. and Unal, G. and Dicle, O. and Selver, M.A.},
keywords = {Segmentation, Challenge, Abdomen, Cross-modality}
}

@InProceedings{10.1007/978-3-030-87602-9_24,
author="Demir, U. and Ozer, A. and Sahin, Y.H. and Unal, G.",
editor="Rekik, I. and Adeli, E. and Park, S.H. and Schnabel, J.",
title="Uncertainty-Based Dynamic Graph Neighborhoods for Medical Segmentation",
booktitle="Predictive Intelligence in Medicine",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="255--265",
abstract="In recent years, deep learning based methods have shown success in essential medical image analysis tasks such as segmentation. Post-processing and refining the results of segmentation is a common practice to decrease the misclassifications originating from the segmentation network. In addition to widely used methods like Conditional Random Fields (CRFs) which focus on the structure of the segmented volume/area, a graph-based recent approach makes use of certain and uncertain points in a graph and refines the segmentation according to a small graph convolutional network (GCN). However, there are two drawbacks of the approach: most of the edges in the graph are assigned randomly and the GCN is trained independently from the segmentation network. To address these issues, we define a new neighbor-selection mechanism according to feature distances and combine the two networks in the training procedure. According to the experimental results on pancreas segmentation from Computed Tomography (CT) images, we demonstrate improvement in the quantitative measures. Also, examining the dynamic neighbors created by our method, edges between semantically similar image parts are observed. The proposed method also shows qualitative enhancements in the segmentation maps, as demonstrated in the visual results.",
isbn="978-3-030-87602-9"
}


@inproceedings{mertan2020,
  author    = {Alican, M. and Sahin, Y.H. and Duff, D.J. and Unal, G.},
  title     = {A new distributional ranking loss with uncertainty: illustrated in relative depth estimation},
  booktitle = {International Conference on 3D Vision (3DV)},
  publisher = {IEEE},
  year      = {2020},
  pages     = {1079-1088}
}

@ARTICLE{Kafieh,
  author={Kafieh, R. and Rabbani, H. and Unal, G.},
  journal={IEEE Access}, 
  title={Bandlets on Oriented Graphs: Application to Medical Image Enhancement}, 
  year={2019},
  volume={7},
  number={},
  pages={32589-32601},
  doi={10.1109/ACCESS.2019.2903467}}


@ARTICLE{Trapl,
  author={Trapl, D and Horvacanin, I and Mareska, V and Ozcelik, F and Unal, G. and Spiwok, V.},
  journal={Front. Mol. Biosci.}, 
  title={Anncolvar: Approximation of Complex Collective Variables by Artificial Neural Networks for Analysis and Biasing of Molecular Simulations.}, 
  year={2019},
  volume={6},
  number={25},
  doi={10.3389/fmolb.2019.00025}}

@article{CETINKARAYUMAK2018145,
title = {Asymmetric Orientation Distribution Functions ({AODFs}) revealing intravoxel geometry in diffusion {MRI}},
journal = {Magnetic Resonance Imaging},
volume = {49},
pages = {145-158},
year = {2018},
issn = {0730-725X},
doi = {https://doi.org/10.1016/j.mri.2018.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0730725X18300316},
author = {{Cetin Karayumak},S. and Özarslan, E. and Unal, G.},
keywords = {ODF regularization, Asymmetric fiber orientations, Fiber asymmetry measure, Asymmetric ODF (AODF), Directional spatial filtering, Steerable filtering, Diffusion MRI, HARDI},
abstract = {Characterization of anisotropy via diffusion MRI reveals fiber crossings in a substantial portion of voxels within the white-matter (WM) regions of the human brain. A considerable number of such voxels could exhibit asymmetric features such as bends and junctions. However, widely employed reconstruction methods yield symmetric Orientation Distribution Functions (ODFs) even when the underlying geometry is asymmetric. In this paper, we employ inter-voxel directional filtering approaches through a cone model to reveal more information regarding the cytoarchitectural organization within the voxel. The cone model facilitates a sharpening of the ODFs in some directions while suppressing peaks in other directions, thus yielding an Asymmetric ODF (AODF) field. We also show that a scalar measure of AODF asymmetry can be employed to obtain new contrast within the human brain. The feasibility of the technique is demonstrated on in vivo data obtained from the MGH-USC Human Connectome Project (HCP) and Parkinson's Progression Markers Initiative (PPMI) Project database. Characterizing asymmetry in neural tissue cytoarchitecture could be important for localizing and quantitatively assessing specific neuronal pathways.}
}

@article{Ugurlu2018,
title = "Neighborhood resolved fiber orientation distributions in automatic labeling of white matter fiber pathways",
journal = "Medical Image Analysis",
volume = "46",
pages = "130 - 145",
year = "2018",
issn = "1361-8415",
author = "Ugurlu, D. and Firat, Z. and Türe, U. and Unal, G.",
}

@book{Rekik2018,
year={2018},
isbn={978-3-030-00319-7},
booktitle={PRedictive Intelligence in Medicine PRIME.},
volume={11121},
series={Lecture Notes in Computer Science LNCS},
editor={Rekik, I. and Unal, G. and Adeli, E. and Park, S.H.},
title={PRedictive Intelligence in Medicine PRIME. Held in Conjunction with MICCAI 2018, Granada, Spain, Proceedings},
publisher={Springer Cham}
}

@article{doi:10.1080/01431161.2018.1425561,
author = {Tuna, C. and Unal, G. and Sertel, E.},
title = {Single-frame super resolution of remote-sensing images by convolutional neural networks},
journal = {International Journal of Remote Sensing},
volume = {39},
number = {8},
pages = {2463-2479},
year  = {2018},
publisher = {Taylor \& Francis},
doi = {10.1080/01431161.2018.1425561},

URL = { 
        https://doi.org/10.1080/01431161.2018.1425561
    
},
eprint = { 
        https://doi.org/10.1080/01431161.2018.1425561
}}

@article{Aksoy_2017,
doi = {10.1088/1361-6560/aa9474},
url = {https://dx.doi.org/10.1088/1361-6560/aa9474},
year = {2017},
publisher = {IOP Publishing},
volume = {62},
number = {24},
pages = {9377},
author = {Aksoy, T. and Špiclin, Ž. and Pernuš, F. and Unal, G.  },
title = {Monoplane 3D–2D registration of cerebral angiograms based on multi-objective stratified optimization},
journal = {Physics in Medicine \& Biology},
abstract = {Registration of 3D pre-interventional to 2D intra-interventional medical images has an increasingly important role in surgical planning, navigation and treatment, because it enables the physician to co-locate depth information given by pre-interventional 3D images with the live information in intra-interventional 2D images such as x-ray. Most tasks during image-guided interventions are carried out under a monoplane x-ray, which is a highly ill-posed problem for state-of-the-art 3D to 2D registration methods. To address the problem of rigid 3D–2D monoplane registration we propose a novel multi-objective stratified parameter optimization, wherein a small set of high-magnitude intensity gradients are matched between the 3D and 2D images. The stratified parameter optimization matches rotation templates to depth templates, first sampled from projected 3D gradients and second from the 2D image gradients, so as to recover 3D rigid-body rotations and out-of-plane translation. The objective for matching was the gradient magnitude correlation coefficient, which is invariant to in-plane translation. The in-plane translations are then found by locating the maximum of the gradient phase correlation between the best matching pair of rotation and depth templates. On twenty pairs of 3D and 2D images of ten patients undergoing cerebral endovascular image-guided intervention the 3D to monoplane 2D registration experiments were setup with a rather high range of initial mean target registration error from 0 to 100 mm. The proposed method effectively reduced the registration error to below 2 mm, which was further refined by a fast iterative method and resulted in a high final registration accuracy (0.40 mm) and high success rate (96%). Taking into account a fast execution time below 10 s, the observed performance of the proposed method shows a high potential for application into clinical image-guidance systems.}
}

@article{STAJDUHAR2017151,
title = {Semi-automated detection of anterior cruciate ligament injury from {MRI}},
journal = {Computer Methods and Programs in Biomedicine},
volume = {140},
pages = {151-164},
year = {2017},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0169260716305028},
author = {Štajduhar, I. and Mamula, M. and Miletić, D. and Unal, G. },
keywords = {Anterior cruciate ligament (ACL) injury, Knee joint MRI, Feature extraction, Machine learning, Computer-aided diagnosis},
abstract = {Background and objectives: A radiologist’s work in detecting various injuries or pathologies from radiological scans can be tiresome, time consuming and prone to errors. The field of computer-aided diagnosis aims to reduce these factors by introducing a level of automation in the process. In this paper, we deal with the problem of detecting the presence of anterior cruciate ligament (ACL) injury in a human knee. We examine the possibility of aiding the diagnosis process by building a decision-support model for detecting the presence of milder ACL injuries (not requiring operative treatment) and complete ACL ruptures (requiring operative treatment) from sagittal plane magnetic resonance (MR) volumes of human knees. Methods: Histogram of oriented gradient (HOG) descriptors and gist descriptors are extracted from manually selected rectangular regions of interest enveloping the wider cruciate ligament area. Performance of two machine-learning models is explored, coupled with both feature extraction methods: support vector machine (SVM) and random forests model. Model generalisation properties were determined by performing multiple iterations of stratified 10-fold cross validation whilst observing the area under the curve (AUC) score. Results: Sagittal plane knee joint MR data was retrospectively gathered at the Clinical Hospital Centre Rijeka, Croatia, from 2007 until 2014. Type of ACL injury was established in a double-blind fashion by comparing the retrospectively set diagnosis against the prospective opinion of another radiologist. After clean up, the resulting dataset consisted of 917 usable labelled exam sequences of left or right knees. Experimental results suggest that a linear-kernel SVM learned from HOG descriptors has the best generalisation properties among the experimental models compared, having an area under the curve of 0.894 for the injury-detection problem and 0.943 for the complete-rupture-detection problem. Conclusions: Although the problem of performing semi-automated ACL-injury diagnosis by observing knee-joint MR volumes alone is a difficult one, experimental results suggest potential clinical application of computer-aided decision making, both for detecting milder injuries and detecting complete ruptures.}
}

@article{GULER201679,
title = {Landmarks inside the shape: Shape matching using image descriptors},
journal = {Pattern Recognition},
volume = {49},
pages = {79-88},
year = {2016},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2015.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0031320315002733},
author = {Guler, R.A. and Tari, S. and Unal, G.},
keywords = {Shape retrieval, Shape matching, Internal landmarks, SIFT, Screened Poisson hyper-fields, Screened Poisson Encoding Maps (SPEM), Shape silhouettes},
abstract = {In the last few decades, significant advances in image matching are provided by rich local descriptors that are defined through physical measurements such as reflectance. As such measurements are not naturally available for silhouettes, existing arsenal of image matching tools cannot be utilized in shape matching. We propose that the recently presented SPEM representation can be used analogous to image intensities to detect local keypoints using invariant image salient point detectors. We devise a shape similarity measure based on the number of matching internal regions. The performance of the similarity measure in planar shape retrieval indicates that the landmarks inside the shape silhouettes provide a strong representation of the regional characteristics of 2D planar shapes.}
}
@book{miccai2016,
year={2016},
isbn={978-3-030-00319-7},
booktitle={Series: Image Processing, Computer Vision, Pattern Recognition, and Graphics.},
volume={9900-01-02},
series={Lecture Notes in Computer Science LNCS},
editor={Ourselin, S. and Joskowicz, L. and Sabuncu, M.R. and Unal, G. and Wells, W.},
title={Proceedings of the Medical Image Computing and Computer-Assisted Intervention – MICCAI Athens, Greece},
publisher={Springer International Publishing}
}